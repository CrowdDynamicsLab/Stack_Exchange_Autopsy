\section{Evaluating Our Proposed Models}
In this section we identify optimal models (basis and interaction) from three different perspectives: the accuracy of fitting content generation time series observed in our dataset (Section 6.1), the performance of predicting content volume in long run (Section 6.2), and the perplexity of characterizing content generation dynamics at early stage (Section 6.3).

\subsection{Model Fitting}
We fit each variant of production model (basis and interaction) for each content type to the observed content generation time series (monthly granularity) in each Stack Exchange website. Notice that among the different variants of production models, the models using power or exponent basis have a parsimonious set of parameters. For example, answer generation model using power basis function requires only three parameters for interactive essential interaction (See Section 4.2), and four parameters for remaining interaction types. In contrast, answer generation model using sigmoid basis function requires five parameters for interactive essential interaction, and six parameters for remaining interaction types. 

\textbf{Parameter Estimation.} Our parameter learning process has three sequential steps enforced by the content dependency among question, answer and comment; we first learn the best-fit parameters for modeling question, followed by answer, followed by comment. At each step, we use the parameters learnt in earlier steps to generate input.

We restrict some parameters of our production models to be non-negative, e.g., non-negative exponents in power basis. These restrictions are important because the underlying factors positively affect the output. We use the trust-region reflective algorithm to solve our constrained least square optimization problem. The algorithm is appropriate for solving non-linear least squares problems with constraints.

\textbf{Evaluation Method.} We evaluate the overall fitting accuracy using four metrics: root mean square error (RMSE), normalized root mean square Error (NRMSE), explained variance score (EVS), and Akaike information criterion (AIC). Given two series, the observed series for content $w$, $N_w(t)$, and the prediction $\hat{N_w(t)}$ of the series by a model with $k$ parameters, we compute these metrics as follows: RMSE = $\sqrt{\frac{1}{T}\sum_{t=1}^{T}(N_w(t)-\hat{N_w(t)})^2}$; NRMSE = $\frac{RMSE}{max(N_w(t))-min(N_w(t))}$; EVS = $1-\frac{Var(N_w(t)-\hat{N_w(t)})}{Var(N_w(t))}$; AIC = $T*ln(\frac{1}{T}\sum_{t=1}^{T}(N_w(t)-\hat{N_w(t)})^2)+2k$. Among the four metrics, RMSE and NRMSE are error metrics (low value implies good fit), AIC is an information theoretic metric to capture the trade-off between model complexity and goodness-of-fit (low value implies good model), and EVS refers to a model's ability to capture variance in data (high value implies good model). All four metrics are consistent with the non-linear least squares problem. 

\textbf{Fitting Results.} Now, we compare the fitting accuracy of production models for all markets in Stack Exchange, using the four metrics, each summarized via mean, for each content type (question, answer and comment). We found that the models with exponential and sigmoid basis do not fit the data for many Stack Exchange. Accordingly, in Table~\ref{tbl:model_fit}, we only present the results for production models with power basis and different interaction types. Notice that the models with interactive essential interaction outperform the remaining models for all metrics and content types. We performed paired t-tests to determine if the improvements for interactive essential interaction are statistically significant; the results are positive with $p<0.01$.

\begin{table}[ht]
	\vspace{-0.5\baselineskip}
	\caption{The comparison of fitting accuracy of production models (with power basis and different interaction types) for all Stack Exchange. The models with interactive essential interaction outperform the remaining models for all metrics and content types. The improvements for interactive essential interaction are statistically significant---validated through paired t-tests.}
    \vspace{-\baselineskip}
	\label{tbl:model_fit}
	\begin{center}
	\begin{tabular}{llcccc}
    \toprule
    \multirow{2}{*}{Content} & Interaction & Avg. & Avg. & Avg. & Avg.\\
    & Type & RMSE & NRMSE & EVS & AIC\\
    \midrule
    Question & Single Factor & 25.742 & 0.086 & 0.791 & 104.473\\
    \midrule
    \multirow{4}{*}{Answer} & Essential & 70.307 & 0.092 & 0.789 & 208.820\\
    & I. Essential & \textbf{64.624} & \textbf{0.083} & \textbf{0.825} & \textbf{196.395}\\
    & Antagonistic & 72.765 & 0.094 &  0.778 & 210.958\\
    & Substitutable & 68.900 & 0.089 & 0.805 & 207.609\\
    \midrule
    \multirow{4}{*}{Comment} & Essential & 146.644 & 0.084 & 0.833 & 328.245\\
    & I. Essential & \textbf{137.228} & \textbf{0.081} & \textbf{0.845} & \textbf{318.243}\\
    & Antagonistic & 155.969 & 0.088 &  0.818 & 334.118\\
    & Substitutable & 155.433 & 0.089 & 0.820 & 335.102\\
    \bottomrule
	\end{tabular}
	\end{center}
    \vspace{-\baselineskip}
\end{table}

\subsection{Forecasting Content Generation} 
We apply production models with power basis and interactive essential interaction to forecast content volume in long run---one year ahead in future. Specifically, we train a model using the first 12 months, and examine how well the model forecasts content dynamics in next 12 months; with monthly granularity. We validate the forecasting capability by examining the overall prediction error (NRMSE) into the future. 

We summarize the prediction NRMSE for all Stack Exchange using mean and variance--- (i) question: 0.11 (mean), 0.08 (variance); (ii) answer: 0.12 (mean), 0.09 (variance); (iii) comments: 0.11 (mean) 0.10 (variance). We performed these experiments for different time granularity, e.g., week, month, quarter, and found consistent conclusion. We do not report these results for brevity.

\subsection{Parameter Estimation for New Markets} 
To better predict the success or failure of new markets (6-12 months old), we use model parameters learnt from old markets (at least 36 months old) as priors for new markets. We validate the parameter estimation capability by first training models using first 6 months of new market, and the priors based on the nearest (based on user size) old market, and then examining how well the model forecasts content dynamics in remaining months. 

We summarize the prediction NRMSE for new Stack Exchange using mean and variance--- (i) question: 0.1 (mean), 0.12 (variance); (ii) answer: 0.09 (mean), 0.08 (variance); (iii) comments: 0.14 (mean), 0.13 (variance).

