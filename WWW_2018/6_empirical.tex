\section{Evaluating Our Proposed Models}
In this section we identify optimal models (basis and interaction) from three different perspectives: the accuracy of fitting content generation time series observed in our dataset (Section 6.1), the performance of predicting content volume in long run (Section 6.2), and the perplexity of characterizing content generation dynamics at early stage (Section 6.3).

\subsection{Model Fitting}
We fit each variant of production model (basis and interaction) for each content type to the observed content generation time series (monthly granularity) in each Stack Exchange. Notice that among the different variants of production models, the models using power or exponent basis have a parsimonious set of parameters. For example, answer generation model using power basis function requires only three parameters for interactive essential interaction (See Section 4.2), and four parameters for remaining interaction types. In contrast, answer generation model using sigmoid basis function requires five parameters for interactive essential interaction, and six parameters for remaining interaction types. 

\textbf{Parameter Estimation.} Our parameter learning process has three sequential steps enforced by the content dependency among question, answer and comment; we first learn the best-fit parameters for modeling question, followed by answer, followed by comment. At each step, we use the parameters learnt in earlier steps to generate input.

We restrict some parameters of our production models to be non-negative, e.g., non-negative exponents in power basis. These restrictions are important because the underlying factors positively affect the output. We use the trust-region reflective algorithm to solve our constrained least square optimization problem. The algorithm is appropriate for solving non-linear least squares problems with constraints.

\textbf{Evaluation Method.} We evaluate the overall fitting accuracy using four metrics: root mean square error (RMSE), normalized root mean square Error (NRMSE), explained variance score (EVS), and Akaike information criterion (AIC). Given two series, the observed series for content $w$, $N_w(t)$, and the prediction $\hat{N_w(t)}$ of the series by a model with $k$ parameters, we compute these metrics as follows: RMSE = $\sqrt{\frac{1}{T}\sum_{t=1}^{T}(N_w(t)-\hat{N_w(t)})^2}$; NRMSE = $\frac{RMSE}{max(N_w(t))-min(N_w(t))}$; EVS = $1-\frac{Var(N_w(t)-\hat{N_w(t)})}{Var(N_w(t))}$; AIC = $T*ln(\frac{1}{T}\sum_{t=1}^{T}(N_w(t)-\hat{N_w(t)})^2)+2k$. Among the four metrics, RMSE and NRMSE are error metrics (low value implies good fit), AIC is an information theoretic metric to capture the trade-off between model complexity and goodness-of-fit (low value implies good model), and EVS refers to a model's ability to capture variance in data (high value implies good model). All four metrics are consistent with the non-linear least squares problem. 

\textbf{Fitting Results.} Now, we compare the fitting accuracy of production models for all markets in Stack Exchange, using the four metrics, each summarized via mean across all Stack Exchange, for each content type. We use content generation time series with monthly granularity as observed data. We found that the models with exponential and sigmoid basis do not fit the data for many Stack Exchange. Accordingly, in Table~\ref{tbl:model_fit}, we only present the results for production models with power basis and different interaction types. Notice that the models with interactive essential interaction outperform the remaining models for all metrics and content types. We performed paired t-tests to determine if the improvements for interactive essential interaction are statistically significant; the results are positive with $p<0.01$.

\begin{table}[ht]
	\vspace{-0.5\baselineskip}
	\caption{The comparison of fitting accuracy of production models (with power basis and different interaction types) for all Stack Exchange. The models with interactive essential interaction outperform the remaining models for all metrics and content types. The improvements for interactive essential interaction are statistically significant---validated via paired t-tests ($p<0.01$).}
    \vspace{-\baselineskip}
	\label{tbl:model_fit}
	\begin{center}
	\begin{tabular}{llcccc}
    \toprule
    \multirow{2}{*}{Content} & Interaction & Avg. & Avg. & Avg. & Avg.\\
    & Type & RMSE & NRMSE & EVS & AIC\\
    \midrule
    Question & Single Factor & 25.742 & 0.086 & 0.791 & 104.473\\
    \midrule
    \multirow{4}{*}{Answer} & Essential & 70.307 & 0.092 & 0.789 & 208.820\\
    & I. Essential & \textbf{64.624} & \textbf{0.083} & \textbf{0.825} & \textbf{196.395}\\
    & Antagonistic & 72.765 & 0.094 &  0.778 & 210.958\\
    & Substitutable & 68.900 & 0.089 & 0.805 & 207.609\\
    \midrule
    \multirow{4}{*}{Comment} & Essential & 146.644 & 0.084 & 0.833 & 328.245\\
    & I. Essential & \textbf{137.228} & \textbf{0.081} & \textbf{0.845} & \textbf{318.243}\\
    & Antagonistic & 155.969 & 0.088 &  0.818 & 334.118\\
    & Substitutable & 155.433 & 0.089 & 0.820 & 335.102\\
    \bottomrule
	\end{tabular}
	\end{center}
    \vspace{-\baselineskip}
\end{table}

Thus we use production models with power basis and interactive essential interaction for prediction tasks.

\subsection{Forecasting Content Generation} 
We apply production models with power basis and interactive essential interaction to forecast content volume in long run---one year ahead in future. Specifically, we train each model using the content generation data from first 12 months (beyond the ramp period), and then examine how well the model forecasts content dynamics in next 12 months. We validate the forecasting capability by examining the overall prediction error (NRMSE) into the future. 

We compute the prediction NRMSE across all Stack Exchange, and summarize the results using mean ($\mu$) and variance ($\sigma$)--- (i) question: $\mu = 0.11$, $\sigma = 0.09$; (ii) answer: $\mu = 0.12$, $\sigma = 0.09$; (iii) comments: $\mu = 0.11$, $\sigma = 0.19$. Notice that our models can forecast future content dynamics with high accuracy. We performed these experiments for different time granularity, e.g., week, month, quarter, and found consistent conclusion. We do not report these results for brevity.

\subsection{Parameter Estimation for New Markets} 
To better predict the success or failure of young markets (6-12 months old), we use model parameters learnt from mature markets (at least 36 months old) as informative priors for new markets. We incorporate these informative priors using Bayesian parameter estimation technique. We validate the parameter estimation capability by first training models using the first 6 month's data from young market, and the informative priors based on the nearest (based on user size) mature market, and then examining how well the model forecasts content dynamics in remaining months. 

We compute the prediction NRMSE across all young Stack Exchange, and summarize the results using mean ($\mu$) and variance ($\sigma$)--- (i) question: $\mu = 0.10$, $\sigma = 0.12$; (ii) answer: $\mu = 0.09$, $\sigma = 0.08$; (iii) comments: $\mu = 0.14$, $\sigma = 0.13$.

